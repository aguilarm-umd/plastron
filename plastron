#!/usr/bin/env python3
#-*- coding: utf-8 -*-

from __future__ import print_function

import argparse
import csv
from fractions import gcd
from importlib import import_module
import os.path
import sys
import yaml
import re
import logging
import logging.config
from datetime import datetime
from classes import pcdm,util
from classes.exceptions import ConfigException, DataReadException, RESTAPIException
from time import sleep
import namespaces
from rdflib.util import from_n3
from classes.util import get_title_string
from handler import ndnp

logger = logging.getLogger(__name__)
now = datetime.utcnow().strftime('%Y%m%d%H%M%S')

#============================================================================
# HELPER FUNCTIONS
#============================================================================

def print_header():
    '''Common header formatting.'''
    title = '|     PLASTRON     |'
    bar = '+' + '='*(len(title)-2) + '+'
    spacer = '|' + ' '*(len(title)-2) + '|'
    print('\n'.join(['', bar, spacer, title, spacer, bar, '']))

def print_footer():
    '''Report success or failure and resources created.'''
    print('\nScript complete. Goodbye!\n')

def parse_predicate_list(string, delimiter=','):
    manager = namespaces.get_manager()
    return [ from_n3(p, nsm=manager) for p in string.split(delimiter) ]

def test_connection(fcrepo):
    # test connection to fcrepo
    logger.debug("fcrepo.endpoint = %s", fcrepo.fullpath)
    logger.debug("fcrepo.relpath = %s", fcrepo.fullpath)
    logger.debug("fcrepo.fullpath = %s", fcrepo.fullpath)
    logger.info("Testing connection to {0}".format(fcrepo.fullpath))
    if fcrepo.is_reachable():
        logger.info("Connection successful.")
    else:
        logger.warn("Unable to connect.")
        sys.exit(1)

# custom argument type for percentage loads
def percentage(n):
    p = int(n)
    if not p > 0 and p < 100:
        raise argparse.ArgumentTypeError("Percent param must be 1-99")
    return p

#============================================================================
# MAIN LOOP
#============================================================================

def main():
    '''Parse args and handle options.'''

    parser = argparse.ArgumentParser(
        description='A configurable batch loader for Fedora 4.'
        )
    common_required = parser.add_argument_group('required arguments')
    common_required.add_argument('-r', '--repo',
                        help='Path to repository configuration file.',
                        action='store',
                        required=True
                        )
    parser.add_argument('-v', '--verbose',
                        help='increase the verbosity of the status output',
                        action='store_true'
                        )
    parser.add_argument('-q', '--quiet',
                        help='decrease the verbosity of the status output',
                        action='store_true'
                        )

    subparsers = parser.add_subparsers(dest='command')

    parser_ping = subparsers.add_parser('ping')
    parser_ping.set_defaults(function=ping)

    parser_load = subparsers.add_parser('load')
    required = parser_load.add_argument_group('required arguments')
    required.add_argument('-b', '--batch',
                        help='path to batch configuration file',
                        action='store',
                        required=True
                        )
    parser_load.add_argument('-d', '--dryrun',
                        help='iterate over the batch without POSTing',
                        action='store_true'
                        )
    # useful for testing when file loading is too slow
    parser_load.add_argument('-n', '--nobinaries',
                        help='iterate without uploading binaries',
                        action='store_true'
                        )
    parser_load.add_argument('-l', '--limit',
                        help='limit the load to a specified number of top-level objects',
                        action='store',
                        type=int,
                        default=None
                        )
    # load an evenly-spaced percentage of the total batch
    parser_load.add_argument('-%', '--percent',
                        help='load specified percentage of total items',
                        action='store',
                        type=percentage,
                        default=None
                        )
    parser_load.add_argument('--noannotations',
                        help='iterate without loading annotations (e.g. OCR)',
                        action='store_true'
                        )
    parser_load.add_argument('--ignore', '-i',
                        help='file listing items to ignore',
                        action='store'
                        )
    parser_load.add_argument('--wait', '-w',
                        help='wait n seconds between items',
                        action='store'
                        )
    parser_load.set_defaults(function=load)

    parser_list = subparsers.add_parser('list', aliases=['ls'])
    # long mode to print more than just the URIs (name modeled after ls -l)
    parser_list.add_argument('-l', '--long',
                        help='Display additional information besides the URI',
                        action='store_true'
                        )
    parser_list.add_argument('-R', '--recursive',
                        help='List additional objects found by traversing the given predicate(s)',
                        action='store'
                        )
    parser_list.add_argument('uris', nargs='*',
                        help='URIs of repository objects to list'
                        )
    parser_list.set_defaults(function=ls)

    parser_mkcol = subparsers.add_parser('mkcol')
    parser_mkcol.add_argument('-n', '--name',
                        help='Name of the collection.',
                        action='store',
                        required=True
                        )
    # if given, will write the collection URI to it
    parser_mkcol.add_argument('-b', '--batch',
                        help='Path to batch configuration file.',
                        action='store'
                        )
    parser_mkcol.set_defaults(function=mkcol)

    parser_delete = subparsers.add_parser('delete', aliases=['del', 'rm'])
    parser_delete.add_argument('-R', '--recursive',
                        help='Delete additional objects found by traversing the given predicate(s)',
                        action='store'
                        )
    parser_delete.add_argument('-d', '--dryrun',
                        help='Simulate a delete without modifying the repository',
                        action='store_true'
                        )
    parser_delete.add_argument('-f', '--file',
                        help='File containing a list of URIs to delete',
                        action='store'
                        )
    parser_delete.add_argument('uris', nargs='*',
                        help='Repository URIs to be deleted.'
                        )
    parser_delete.set_defaults(function=delete)

    parser_extractocr = subparsers.add_parser('extractocr')
    parser_extractocr.add_argument('--ignore', '-i',
                        help='file listing items to ignore',
                        action='store'
                        )
    parser_extractocr.set_defaults(function=extractocr)

    # parse command line args
    args = parser.parse_args()

    # load required repository config file and create repository object
    with open(args.repo, 'r') as repo_config_file:
        repo_config = yaml.safe_load(repo_config_file)
        fcrepo = pcdm.Repository(repo_config)

    # get basic logging options
    with open('config/logging.yml', 'r') as configfile:
        logging_options = yaml.safe_load(configfile)

    # log file configuration
    log_dirname = repo_config.get('LOG_DIR')
    log_filename = 'plastron.{0}.{1}.log'.format(args.command, now)
    logfile = os.path.join(log_dirname, log_filename)
    logging_options['handlers']['file']['filename'] = logfile

    # manipulate console verbosity
    if args.verbose:
        logging_options['handlers']['console']['level'] = 'DEBUG'
    elif args.quiet:
        logging_options['handlers']['console']['level'] = 'WARNING'

    # configure logging
    logging.config.dictConfig(logging_options)

    logger.info('Loaded repo configuration from {0}'.format(args.repo))

    # dispatch to the selected subcommand
    args.function(fcrepo, args)

############
# COMMANDS #
############

def ping(fcrepo, args):
    test_connection(fcrepo)

def load_item(fcrepo, item, args, extra=None):
    # read data for item
    logger.info('Reading item data')
    item.read_data()

    # open transaction
    logger.info('Opening transaction')
    fcrepo.open_transaction()

    # create item and its components
    try:
        keep_alive = pcdm.TransactionKeepAlive(fcrepo, 90)
        keep_alive.start()

        logger.info('Creating item')
        item.recursive_create(fcrepo)
        logger.info('Creating ordering proxies')
        item.create_ordering(fcrepo)
        if not args.noannotations:
            logger.info('Creating annotations')
            item.create_annotations(fcrepo)

        if extra:
            logger.info('Adding additional triples')
            if re.search(r'\.(ttl|n3|nt)$', extra):
                rdf_format = 'n3'
            elif re.search(r'\.(rdf|xml)$', extra):
                rdf_format = 'xml'
            item.add_extra_properties(extra, rdf_format)

        logger.info('Updating item and components')
        item.recursive_update(fcrepo)
        if not args.noannotations:
            logger.info('Updating annotations')
            item.update_annotations(fcrepo)

        keep_alive.stop()

        # commit transaction
        logger.info('Committing transaction')
        fcrepo.commit_transaction()
        logger.info('Performing post-creation actions')
        item.post_creation_hook()
        return True

    except (RESTAPIException, FileNotFoundError) as e:
        # if anything fails during item creation or commiting the transaction
        # attempt to rollback the current transaction
        # failures here will be caught by the main loop's exception handler
        # and should trigger a system exit
        logger.error("Item creation failed: {0}".format(e))
        fcrepo.rollback_transaction()
        logger.warn('Transaction rolled back. Continuing load.')

    except KeyboardInterrupt as e:
        # set the stop flag on the keep-alive ping
        keep_alive.stop()
        logger.error("Load interrupted")
        sys.exit(2)

def load(fcrepo, args):
    if not args.quiet:
        print_header()

    # Load batch configuration
    with open(args.batch, 'r') as batch_config:
        batch_options = yaml.safe_load(batch_config)
        log_location = batch_options.get('LOG_LOCATION')

    logger.info('Loaded batch configuration from {0}'.format(args.batch))

    if args.nobinaries:
        fcrepo.load_binaries = False

    # Define the data_handler function for the data being loaded
    logger.info("Initializing data handler")
    module_name = batch_options.get('HANDLER')
    handler = import_module('handler.' + module_name)
    logger.info('Loaded "{0}" handler'.format(module_name))

    # "--nobinaries" implies "--noannotations"
    if args.nobinaries:
        logger.info("Setting --nobinaries implies --noannotations")
        args.noannotations = True

    # Invoke the data handler by calling the load function on the batch config
    try:
        batch = handler.load(fcrepo, batch_options)
    except ConfigException as e:
        logger.error(e.message)
        logger.error(
            "Failed to load batch configuration from {0}".format(args.batch)
            )
        sys.exit(1)

    if not args.dryrun:
        test_connection(fcrepo)

        # read the log of completed items
        mapfile = os.path.join(log_location, batch_options.get('MAPFILE'))
        fieldnames = ['number', 'timestamp', 'title', 'path', 'uri']
        try:
            completed = util.ItemLog(mapfile, fieldnames, 'path')
        except Exception as e:
            logger.error('Non-standard map file specified: {0}'.format(e))
            sys.exit(1)

        logger.info('Found {0} completed items'.format(len(completed)))

        if args.ignore is not None:
            try:
                ignored = util.ItemLog(args.ignore, fieldnames, 'path')
            except Exception as e:
                logger.error('Non-standard ignore file specified: {0}'.format(e))
                sys.exit(1)
        else:
            ignored = []

        skipfile = os.path.join(log_location, 'skipped.load.{0}.csv'.format(now))
        skipped = util.ItemLog(skipfile, fieldnames, 'path')

        # set up interval from percent parameter and store set of items to load
        if args.percent is not None:
            gr_common_div = gcd(100, args.percent)
            denom = int(100 / gr_common_div)
            numer = int(args.percent / gr_common_div)
            logger.info('Loading {0} of every {1} items (= {2}%)'.format(
                            numer, denom, args.percent
                            ))
            load_set = set()
            for i in range(0, batch.length, denom):
                load_set.update(range(i, i + numer))
            logger.info(
                'Items to load: {0}'.format(
                    ', '.join([str(s + 1) for s in sorted(load_set)])
                    ))

        # create all batch objects in repository
        for n, item in enumerate(batch):
            is_loaded = False

            if args.percent is not None and n not in load_set:
                logger.info(
                    'Loading {0} percent, skipping {1}'.format(args.percent, n)
                    )
                continue

            # handle load limit parameter
            if args.limit is not None and n >= args.limit:
                logger.info("Stopping after {0} item(s)".format(args.limit))
                break
            elif item.path in completed:
                continue
            elif item.path in ignored:
                logger.debug('Ignoring {0}'.format(item.path))
                continue

            logger.info(
                "Processing item {0}/{1}...".format(n+1, batch.length)
                )
            if args.verbose:
                item.print_item_tree()

            try:
                logger.info('Loading item {0}'.format(n+1))
                is_loaded = load_item(
                    fcrepo, item, args, extra=batch_options.get('EXTRA')
                    )
            except RESTAPIException as e:
                logger.error(
                    "Unable to commit or rollback transaction, aborting"
                    )
                sys.exit(1)
            except DataReadException as e:
                logger.error(
                    "Skipping item {0}: {1}".format(n + 1, e.message)
                    )

            row = {'number': n + 1,
                   'path': item.path,
                   'timestamp': getattr(
                        item, 'creation_timestamp', str(datetime.utcnow())
                        ),
                   'title': getattr(item, 'title', 'N/A'),
                   'uri': getattr(item, 'uri', 'N/A')
                   }

            # write item details to relevant summary CSV
            if is_loaded:
                completed.writerow(row)
            else:
                skipped.writerow(row)

            if args.wait:
                logger.info("Pausing {0} seconds".format(args.wait))
                sleep(int(args.wait))

    if not args.quiet:
        print_footer()

def ls(fcrepo, args):
    if args.recursive is not None:
        args.predicates = parse_predicate_list(args.recursive)
        logger.info('Listing will traverse the following predicates: {0}'.format(
            ', '.join([ p.n3() for p in args.predicates ]))
            )
    else:
        args.predicates = []

    for item_uri in args.uris:
        for (uri, graph) in fcrepo.recursive_get(item_uri, traverse=args.predicates):
            if args.long:
                title = get_title_string(graph)
                print("{0} {1}".format(uri, title))
            else:
                print(uri)

def mkcol(fcrepo, args):
    # open transaction
    logger.info('Opening transaction')
    fcrepo.open_transaction()

    try:
        collection = pcdm.Collection()
        collection.title = args.name
        collection.create_object(fcrepo)
        collection.update_object(fcrepo)
        # commit transaction
        logger.info('Committing transaction')
        fcrepo.commit_transaction()

    except (RESTAPIException) as e:
        # failures here will be caught by the main loop's exception handler
        # and should trigger a system exit
        logger.error("Error in collection creation: {0}".format(e))

    if args.batch is not None:
        with open(args.batch, 'r') as batchconfig:
            batch = yaml.safe_load(batchconfig)
            batch['COLLECTION'] = str(collection.uri)
        with open(args.batch, 'w') as batchconfig:
            yaml.dump(batch, batchconfig, default_flow_style=False)


def get_uris_to_delete(fcrepo, uri, args):
    if args.recursive is not None:
        logger.info('Constructing list of URIs to delete')
        return fcrepo.recursive_get(uri, traverse=args.predicates)
    else:
        return fcrepo.recursive_get(uri, traverse=[])

def delete_items(fcrepo, uri_list, args):
    if args.dryrun:
        for uri in uri_list:
            for (target_uri, graph) in get_uris_to_delete(fcrepo, uri, args):
                title = get_title_string(graph)
                logger.info("Would delete {0} {1}".format(target_uri, title))
        return True

    # open transaction
    logger.info('Opening transaction')
    fcrepo.open_transaction()

    # delete item
    # (and its components, if a list of predicates to traverse was given)
    try:
        for uri in uri_list:
            for (target_uri, graph) in get_uris_to_delete(fcrepo, uri, args):
                title = get_title_string(graph)
                fcrepo.delete(target_uri)
                logger.info('Deleted resource {0} {1}'.format(target_uri, title))

        # commit transaction
        logger.info('Committing transaction')
        fcrepo.commit_transaction()
        return True

    except RESTAPIException as e:
        # if anything fails during deletion of a set of uris, attempt to
        # rollback the transaction. Failures here will be caught by the main
        # loop's exception handler and should trigger a system exit
        logger.error("Item deletion failed: {0}".format(e))
        fcrepo.rollback_transaction()
        logger.warn('Transaction rolled back.')

def delete(fcrepo, args):
    if not args.quiet:
        print_header()

    if args.recursive is not None:
        logger.info('Recursive delete enabled')
        args.predicates = parse_predicate_list(args.recursive)
        logger.info('Deletion will traverse the following predicates: {0}'.format(
            ', '.join([ p.n3() for p in args.predicates ]))
            )

    test_connection(fcrepo)
    if args.dryrun:
        logger.info('Dry run enabled, no actual deletions will take place')

    try:
        if args.file is not None:
            with open(args.file, 'r') as uri_list:
                delete_items(fcrepo, uri_list, args)
        elif args.uris is not None:
            delete_items(fcrepo, args.uris, args)

    except RESTAPIException as e:
        logger.error(
            "Unable to commit or rollback transaction, aborting"
            )
        sys.exit(1)

    if not args.quiet:
        print_footer()

def extract(fcrepo, uri):
    fcrepo.open_transaction()
    try:
        logger.info("Getting {0} from repository".format(uri))
        page = ndnp.Page.from_repository(fcrepo, uri)
        logger.info("Creating annotations for page {0}".format(page.title))
        for annotation in page.textblocks():
            annotation.create_object(fcrepo)
            annotation.update_object(fcrepo)

        fcrepo.commit_transaction()
        return True

    except (RESTAPIException, DataReadException) as e:
        # if anything fails during item creation or commiting the transaction
        # attempt to rollback the current transaction
        # failures here will be caught by the main loop's exception handler
        # and should trigger a system exit
        logger.error("OCR extraction failed: {0}".format(e))
        fcrepo.rollback_transaction()
        logger.warn('Transaction rolled back. Continuing load.')

def extractocr(fcrepo, args):
    fieldnames = ['uri', 'timestamp']

    # read the log of completed items
    try:
        completed = util.ItemLog('logs/annotated.csv', fieldnames, 'uri')
    except Exception as e:
        logger.error('Non-standard map file specified: {0}'.format(e))
        sys.exit(1)

    logger.info('Found {0} completed items'.format(len(completed)))

    if args.ignore is not None:
        try:
            ignored = util.ItemLog(args.ignore, fieldnames, 'uri')
        except Exception as e:
            logger.error('Non-standard ignore file specified: {0}'.format(e))
            sys.exit(1)
    else:
        ignored = []

    skipfile = 'logs/skipped.extractocr.{0}.csv'.format(now)
    skipped = util.ItemLog(skipfile, fieldnames, 'uri')

    with fcrepo.at_path('/annotations'):
        for line in sys.stdin:
            uri = line.rstrip('\n')
            if uri in completed:
                continue
            elif uri in ignored:
                logger.debug('Ignoring {0}'.format(uri))
                continue

            is_extracted = False
            try:
                is_extracted = extract(fcrepo, uri)
            except RESTAPIException as e:
                logger.error(
                    "Unable to commit or rollback transaction, aborting"
                    )
                sys.exit(1)

            row = {
                'uri': uri,
                'timestamp': str(datetime.utcnow())
                }

            if is_extracted:
                completed.writerow(row)
            else:
                skipped.writerow(row)

if __name__ == "__main__":
    main()
